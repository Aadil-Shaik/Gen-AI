pip install youtube-transcript-api google-generativeai chromadb spacy nltk

python -m spacy download en_core_web_sm

import nltk
nltk.download('punkt_tab')





import os
from typing import List, Dict, Any
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.formatters import TextFormatter
import google.generativeai as genai
import chromadb
from chromadb.utils import embedding_functions
import spacy
import nltk
from nltk.tokenize import sent_tokenize

# Configure API keys
GEMINI_API_KEY = os.getenv('GeminiAPI', 'AIzaSyDy1RrOdnAeX3t0XTLeH_Qlq8XsAloK98M')
genai.configure(api_key=GEMINI_API_KEY)

# Load NLP model for semantic chunking
nlp = spacy.load("en_core_web_sm")

class YouTubeQASystem:
    def _init_(self, db_path="my_vectordb", collection_name="yt_notes_chunked", min_chunk_size=150, max_chunk_size=500):
        # Initialize models
        self.genai_model = genai.GenerativeModel('models/gemini-2.0-flash')
        
        # Adaptive chunking parameters
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
        
        # Initialize vector database
        self.chroma_client = chromadb.PersistentClient(path=db_path)
        self.gemini_ef = embedding_functions.GoogleGenerativeAiEmbeddingFunction(api_key=GEMINI_API_KEY)
        self.collection = self.chroma_client.get_or_create_collection(
            name=collection_name, 
            embedding_function=self.gemini_ef
        )
    
    def _adaptive_chunk_text(self, text: str) -> List[str]:
        """Dynamically adjust chunk size based on content density"""
        doc = nlp(text)
        sentences = [sent.text for sent in doc.sents]
        
        chunks = []
        current_chunk = []
        current_length = 0
        
        for sentence in sentences:
            sentence_length = len(sentence.split())
            
            if sentence_length > self.max_chunk_size:
                chunks.append(sentence)  # Store long sentences as individual chunks
                continue
            
            current_chunk.append(sentence)
            current_length += sentence_length
            
            if current_length >= self.min_chunk_size:
                chunks.append(" ".join(current_chunk))
                current_chunk = []
                current_length = 0
        
        if current_chunk:
            chunks.append(" ".join(current_chunk))
        
        return chunks
    
    def _extract_important_dialogues(self, text: str) -> List[str]:
        """Extract key dialogues that are impactful or philosophical."""
        sentences = sent_tokenize(text)
        important_dialogues = [sent for sent in sentences if len(sent.split()) > 8]
        return important_dialogues

    def _summarize_chunk(self, chunk: str) -> str:
        """Summarize a single chunk using Gemini AI"""
        prompt = "Summarize this section of the transcript while keeping important details: "
        response = self.genai_model.generate_content(prompt + chunk, stream=False)
    
        # Fix: The response structure might have changed
        # Try accessing the text content correctly based on the response structure
        if hasattr(response, 'text'):
            return response.text
        elif hasattr(response, 'parts'):
            return response.parts[0].text
        elif isinstance(response, dict) and 'candidates' in response:
            # For newer API versions that return a dict
            return response['candidates'][0]['content']['parts'][0]['text']
        else:
            # Fallback: convert the entire response to string
            return str(response)       

    def add_video(self, video_id: str, metadata: Dict[str, Any] = None) -> None:
        """Process a YouTube video, chunk its transcript dynamically, summarize chunks, extract key dialogues, and add to the database"""
        try:
            # Get transcript
            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en','en-US','en-GB'])
        
            # Debug the transcript structure
            if transcript:
                print(f"Transcript type: {type(transcript)}")
                if transcript:
                    print(f"First transcript item: {transcript[0]}")
                    print(f"First transcript item type: {type(transcript[0])}")
        
            # Convert transcript to text manually if it's a list of dictionaries
            if isinstance(transcript, list) and transcript and isinstance(transcript[0], dict):
                transcript_text = "\n".join(item.get('text', '') for item in transcript)
            else:
                # Use the formatter as a fallback
                transcript_text = TextFormatter().format_transcript(transcript)

            # Perform adaptive chunking
            chunks = self._adaptive_chunk_text(transcript_text)
            print(f"Split transcript into {len(chunks)} adaptive chunks")

            # Extract key dialogues
            key_dialogues = self._extract_important_dialogues(transcript_text)

            # Debug the response structure
            if chunks:
                test_response = self.genai_model.generate_content("Test prompt", stream=False)
                print(f"Response type: {type(test_response)}")
                print(f"Response dir: {dir(test_response)}")
                print(f"Response repr: {repr(test_response)}")

            # Summarize each chunk
            summaries = [self._summarize_chunk(chunk) for chunk in chunks]
            combined_summary = " ".join(summaries)

            # Prepare for batch insertion
            chunk_ids, chunk_texts, chunk_metadatas = [], [], []
            base_metadata = metadata or {}
            base_metadata["video_id"] = video_id

            for i, chunk in enumerate(summaries):  
                chunk_id = f"{video_id}_{i}"
                chunk_metadata = base_metadata.copy()
                chunk_metadata["chunk_index"] = i
                chunk_metadata["chunk_count"] = len(summaries)

                chunk_ids.append(chunk_id)
                chunk_texts.append(chunk)
                chunk_metadatas.append(chunk_metadata)

            # Insert into database
            self.collection.upsert(
                documents=chunk_texts + key_dialogues,
                metadatas=chunk_metadatas + [{"video_id": video_id, "type": "dialogue"} for _ in key_dialogues],
                ids=chunk_ids + [f"{video_id}dialogue{i}" for i in range(len(key_dialogues))]
            )

            print(f"Successfully added video {video_id} with {len(summaries)} summarized adaptive chunks and {len(key_dialogues)} key dialogues")
        
        except Exception as e:
            print(f"Error processing video {video_id}: {e}")
            # Print more detailed error information
            import traceback
            traceback.print_exc()
    
    def answer_question(self, query: str, n_results: int = 5) -> str:
        """Perform hybrid search with ranking weights and retrieve key dialogues if available"""
        queries = [query]  # Support for multi-part queries in future
        
        all_chunks = []
        used_chunk_ids = set()
        
        for subquery in queries:
            # Semantic Search
            semantic_results = self.collection.query(
                query_texts=[subquery],
                n_results=n_results,
                include=["documents", "metadatas"]
            )
            
            # Keyword Search (BM25-like)
            keyword_results = self.collection.get(
                where_document={"$contains": subquery},
                include=["documents", "metadatas"]
            )
            
            # Retrieve key dialogues related to the topic
            dialogue_results = self.collection.get(
                where={"type": "dialogue"},
                include=["documents", "metadatas"]
            )
            
            # Merge Results with Ranking Weights
            ranked_results = [(doc, 0.7) for doc in semantic_results['documents'][0]]
            ranked_results += [(doc, 0.3) for doc in keyword_results['documents']]
            ranked_results += [(doc, 0.9) for doc in dialogue_results['documents']]  # Prioritize dialogues
            
            # Sort results by weight
            ranked_results.sort(key=lambda x: x[1], reverse=True)
            
            for doc, _ in ranked_results:
                chunk_id = f"{query}_{len(all_chunks)}"
                if chunk_id not in used_chunk_ids:
                    all_chunks.append(doc)
                    used_chunk_ids.add(chunk_id)
        
        # Combine all retrieved chunks as context
        combined_context = "\n\n---\n\n".join(all_chunks)
        
        # Generate response
        prompt = f"""
        QUESTION: {query}
        
        DOCUMENTS:
        {combined_context}
        """
        response = self.genai_model.generate_content(prompt, stream=False)
        
        # Handle the response consistently with the _summarize_chunk method
        if hasattr(response, 'text'):
            return response.text
        elif hasattr(response, 'parts'):
            return response.parts[0].text
        elif isinstance(response, dict) and 'candidates' in response:
            # For newer API versions that return a dict
            return response['candidates'][0]['content']['parts'][0]['text']
        else:
            # Fallback: convert the entire response to string
            return str(response)



# Main execution script
if _name_ == "_main_":
    print("Initializing YouTube QA System...")
    
    # Initialize the system
    qa_system = YouTubeQASystem()
    
    # Add videos (sample IDs)
    sample_videos = [
        "dcXqhMqhZUo",  
        "h6fcK_fRYaI&t=2s",  
        "TeOCNEfwaHM"   
    ]
    
    print(f"Processing {len(sample_videos)} videos...")
    for video_id in sample_videos:
        print(f"\nProcessing video: {video_id}")
        qa_system.add_video(video_id)
    
    print("\nSystem initialization complete.")
    print("You can now ask questions about the videos.")



# Answer a question
question = "Give me some Marcus Aurelius dialogues"
answer = qa_system.answer_question(question)
    
print("Question:", question)
print("\nAnswer:")
print(answer)
